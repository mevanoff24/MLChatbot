comment_id,parent_id,comment,title,score,tags
20329702,20328835,<p>You may want to try to translate your octave code to python and see what's going on. You can also use the python package to do this for you. Check out <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html' rel='nofollow'>scikit-learn</a> on logistic regression. There is also an easy example in this <a href='http://blog.yhathq.com/posts/logistic-regression-and-python.html' rel='nofollow'>blog</a>.</p>,,2,
33617067,33616265,"<p>Yes; it will work fine, so long as you have enough data on each side to properly define the class. The amount you need depends on the classification method you use. In fact, I have a couple of SVM models that work very nicely, trained with nothing but +ve data -- no -ve data at all!</p> <p>For most methods, the lopsided input suggests that you <em>could</em> toss out the 80% of your +ve cases that aren't doing as much to define the boundary. Which 80% will vary with the method. For instance, spectral clustering and k-means will work well enough if you remove 80% evenly spaced (at random is likely to work). Linear SVM works if you keep only the 10% nearest the boundary. Naive Bayes and random forest can also work nicely with a random 80% removal, although any of these that work by successive refinement may converge a little more slowly.</p>",,1,
33623768,33616265,"<p>It all depends on your results. </p> <hr> <p>Extreme scenario aside: Consider the following scenario: You run your model, and observe your error is somewhat low like 5%. However, in reality, this 5% was because you wrongly classified HALF of your negative data (failed to recognize person's name), then obviously 5% error seems much worse now.</p> <hr> <p>One thing you should do is calculate your precision and recall. </p> <p><strong>Precision</strong>: </p> <p>Out of all the words that we predicted are peoples' names, what fraction of them <em>actually</em> are?</p> <p>Precision = # true positives / (# of true positives + # of false positives)</p> <p>High precision (close to 1) means you have few # of false positives. In other words, your model is pulling a Dos Equis: 'I don't always predict positive, but when I do, I get it right'</p> <p>However, high precision does not tell you whether you classified ALL actual positive examples correctly.</p> <hr> <p><strong>Recall</strong>:</p> <p>Of all the examples that are <em>actually</em> peoples' names, what fraction of those did we predict <em>correctly</em>?</p> <p>Recall = # true positives / (# true positives + # false negatives)</p> <p>High recall (close to 1) means we are correctly classifying the positive examples, and moreover, not misclassifying negative examples</p> <p>However, high recall doesn't tell you whether your model misclassified some negative examples as positives (false positives)</p> <hr> <p>Precision and recall can be tradeoffs (could have lots of one at the expense of the other). If so, look into how to calculate F1 score (simple), which determines whether your model has a sufficient amount of precision and recall.</p> <p>A good F1 score is close to 1. This should tell you more about how well your model is classifying text as people names.</p>",,1,
37392337,37391923,"<p>I think it will be more easily, when you post some data from Ron95_Price_Class.xlsx<br> Right now I see, that you are not delete target variable (y), from train data. You can do it by </p> <pre><code>X = data['Date'] #you can use only one bracket if choose only y = data['Ron95_RM'] #column X = data.drop('Ron95_RM') </code></pre>",,0,
37392955,37391923,"<p>If I am not mistaken the 'u' is just notation that the string is a unicode string. I am not sure how you are running your code, but when i test in an ipython notebook or in a windows command prompt I get the following output:</p> <pre><code>The price for Ron95 in next month will be RM [ 'B'] </code></pre> <p>This is perhaps because I ran this in python 3.5 whereas it appears you are still using python < 3.0. </p> <p>It's not that your answer is wrong, you are just getting info about the format of the data. For other questions on this subject see <a href='https://stackoverflow.com/questions/2464959/whats-the-u-prefix-in-a-python-string'>here</a> and <a href='https://stackoverflow.com/questions/11279331/what-does-the-u-symbol-mean-in-front-of-string-values'>here</a>. The python <a href='https://docs.python.org/3/howto/unicode.html' rel='nofollow noreferrer'>how-to on unicode</a> may also be helpful. </p>",,0,
37393830,37391923,"<p>The Predict method as mentioned in the scikit-learn documentation, <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict' rel='nofollow'>http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict</a>, mentions the return of the predict method is array, shape = [n_samples]. So for you the shape is 1x1 array. To get the desired output you ca try 'price[0]'.</p>",,0,
38633268,38621685,"<p>You are not configuring the C parameter - well, technically you are, but only to the default value - which is one of the usual suspects for overfitting. You can have a look at <a href='http://scikit-learn.org/stable/modules/grid_search.html#grid-search' rel='nofollow'>GridSearchCV</a> and play around a bit with several values for the C parameter (e.g. from 10^-5 to 10^5) to see if it eases your problem. Changing the penalty rule to 'l1' might help as well.</p> <p>Besides, there were several challenges with that competition: It is an imbalanced data set, and the distributions between the training set and the private LB were a bit different. All of this if going to play against you, specially when using simple algorithms like LR.</p>",,0,
40868441,40865717,"<p>First of all, double-check if your problem can really be framed as a classification problem or if it should rather be formulated as a regression problem.</p> <p>Assuming you really want to classify your data into the four unique classes present in the <code>Score_Bucket</code> column, why do you think you cannot use multiple columns as predictors? In fact, you are using the last two columns in your example. You can make your code a bit more readable if you consider that <code>sklearn</code> methods directly work with Pandas DataFrames (no need for converting to NumPy arrays):</p> <pre><code>X = dataset[['CourseLoad_RelativeStudy', 'Sleep_Sex']] y = dataset[['Score_Bucket']] logreg = LogisticRegression() logreg.fit(X, y) </code></pre> <p>If you want to select more columns, you can use the <code>loc</code> method:</p> <pre><code>X = dataset.loc[:, 'Healthy':'Sleep_Sex'] </code></pre> <p>You could also select columns by index:</p> <pre><code>X = dataset.iloc[:, 1:] </code></pre> <p>Regarding your second question, I do get different results from the cross-validation procedure depending on which columns I use as features. Just note that you have a very low number of samples (20), which makes your estimated predictions rather variable.</p>",,1,
41124800,40251587,"<p>I assume you are trying to execute the logistic regression example found in the <em>Tensorflow for Machine Intelligence</em> book.</p> <p>When the csv file is loaded, the path is incorrect:</p> <p>Instead of using </p> <p><code>filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + '/' + file_name])</code></p> <p>use </p> <pre><code>filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)]) </code></pre> <p>The first line will load the file '/train.csv' while the second one will load 'yourPath/train.csv'. Due to the fact that the first file doesn't exists, no data to train can be loaded.</p> <p>The source code of all of the book's examples can be found in the following repo:</p> <p><a href='https://github.com/backstopmedia/tensorflowbook' rel='nofollow noreferrer'>https://github.com/backstopmedia/tensorflowbook</a></p>",,0,
44629756,20328835,"<p>In order to implement Logistic Regression, You may consider the following 2 approaches:</p> <ol> <li><p>Consider How Linear Regression Works. Apply Sigmoid Function to the Hypothesis of Linear Regression and run gradient Descent until convergence. OR Apply the Exponential based Softmax function to rule out lower possibility of occurrence. </p> <pre><code>def logistic_regression(x, y,alpha=0.05,lamda=0): ''' Logistic regression for datasets ''' m,n=np.shape(x) theta=np.ones(n) xTrans = x.transpose() oldcost=0.0 value=True while(value): hypothesis = np.dot(x, theta) logistic=hypothesis/(np.exp(-hypothesis)+1) reg = (lamda/2*m)*np.sum(np.power(theta,2)) loss = logistic - y cost = np.sum(loss ** 2) #print(cost) # avg cost per example (the 2 in 2*m doesn't really matter here. # But to be consistent with the gradient, I include it) # avg gradient per example gradient = np.dot(xTrans, loss)/m # update if(reg): cost=cost+reg theta = (theta - (alpha) * (gradient+reg)) else: theta=theta -(alpha/m) * gradient if(oldcost==cost): value=False else: oldcost=cost print(accuracy(theta,m,y,x)) return theta,accuracy(theta,m,y,x) </code></pre></li> </ol>",,1,
46421997,46349963,<p>You can just use a linear function as activation for your last layer. And it is better to normalize your data to have better results.</p>,,0,
48972997,48962697,"<p>At first what is your main question? The best way to define loss function and predict function separately?</p> <p>I looked your code, I think functionality of <code>init_scope</code> is different between <code>Link</code> and <code>Chain</code>. You cannot use it to register learnable parameter in Chain for this purpose. (Your current usage is for the <code>Link</code> and not for the <code>Chain</code>.)</p> <ul> <li><p><code>init_scope</code> in <code>Link</code> is used to register parameter, <a href='https://docs.chainer.org/en/stable/tutorial/function.html#links-that-wrap-functions' rel='nofollow noreferrer'>https://docs.chainer.org/en/stable/tutorial/function.html#links-that-wrap-functions</a> <a href='https://github.com/chainer/chainer/blob/master/chainer/link.py#L197' rel='nofollow noreferrer'>https://github.com/chainer/chainer/blob/master/chainer/link.py#L197</a></p></li> <li><p><code>init_scope</code> in <code>Chain</code> is used to register other <code>links</code>, <a href='https://docs.chainer.org/en/stable/tutorial/basic.html#write-a-model-as-a-chain' rel='nofollow noreferrer'>https://docs.chainer.org/en/stable/tutorial/basic.html#write-a-model-as-a-chain</a> <a href='https://github.com/chainer/chainer/blob/master/chainer/link.py#L675' rel='nofollow noreferrer'>https://github.com/chainer/chainer/blob/master/chainer/link.py#L675</a></p></li> </ul> <p>In your case, I think you can just use <code>chainer.links.Linear</code> to your <code>LogisticRegressionModel</code>, or you can define your own <code>Link</code> class which has the learnable parameter <code>W</code> and use this own <code>link</code> class in your <code>LogisticRegressionModel</code>.</p>",,1,
49137743,49135107,"<p>You can use either GLM.jl (simpler) or Flux.jl (more involved but more powerful in general). In the code I generate the data so that you can check if the result is correct. Additionally I have a binary response variable - if you have other encoding of target variable you might need to change the code a bit.</p> <p>Here is the code to run (you can tweak the parameters to increase the convergence speed - I chose ones that are safe):</p> <pre><code>using GLM, DataFrames, Flux.Tracker srand(1) n = 10000 df = DataFrame(s1=rand(n), s2=rand(n)) df[:y] = rand(n) .< 1 ./ (1 .+ exp.(-(1 .+ 2 .* df[1] .+ 0.5 .* df[2]))) model = glm(@formula(y~s1+s2), df, Binomial(), LogitLink()) x = Matrix(df[1:2]) y = df[3] W = param(rand(2,1)) b = param(rand(1)) predict(x) = 1.0 ./ (1.0+exp.(-x*W .- b)) loss(x,y) = -sum(log.(predict(x[y,:]))) - sum(log.(1 - predict(x[.!y,:]))) function update!(ps, η = .0001) for w in ps w.data .-= w.grad .* η w.grad .= 0 end end i = 1 while true back!(loss(x,y)) max(maximum(abs.(W.grad)), abs(b.grad[1])) > 0.001 || break update!((W, b)) i += 1 end </code></pre> <p>And here are the results:</p> <pre><code>julia> model # GLM result StatsModels.DataFrameRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} Formula: y ~ 1 + s1 + s2 Coefficients: Estimate Std.Error z value Pr(>|z|) (Intercept) 0.910347 0.0789283 11.5338 <1e-30 s1 2.18707 0.123487 17.7109 <1e-69 s2 0.556293 0.115052 4.83513 <1e-5 julia> (b, W, i) # Flux result with number of iterations needed to converge (param([0.910362]), param([2.18705; 0.556278]), 1946) </code></pre>",,3,
