comment_id,parent_id,comment,title,score,tags
20328835,,"<p>I want to implement Logisitic regression from scratch in python. Following are the functions in it: </p> <ol> <li>sigmoid</li> <li>cost </li> <li>fminunc</li> <li>Evaluating Logistic regression</li> </ol> <p>I would like to know, what would be a great start to this to start from scratch in python. Any guidance on how and what would be good. I know the theory of those functions but looking for a better pythonic answer.</p> <p>I used octave and I got it all right but dont know how to start in python as OCtave already has those packages setup to do the work.</p>",Logistic regression using python,1,python statistics machine-learning logistic-regression
33616265,,"<p>I am learning Machine Learning, and so far I have used logistic regression with problems with balanced data like sentiment analysis where I had equal number of training data for both classes (+ve,-ve).</p> <p>Now, I am working on Named Entity Recognition problem where I have to identify names of people in the text. For this my data is sparse, less than 10% of my training data is +ve case(actually a person tag), reset is negative case(not a person tag). So there is massive imbalance in my training data.</p> <p>Will a supervised learning algorithm work in this scenario?</p>",Will logistic regression work with imblanced data?,2,machine-learning logistic-regression
37391923,,"<p>I am currently doing the Logistic Regression in machine learning for python. This is the code i write.</p> <pre><code>import pandas as pd from sklearn import linear_model import numpy as np from sklearn.utils import column_or_1d logistic = linear_model.LogisticRegression() data = pd.read_excel('/home/mick/PycharmProjects/project1/excel/Ron95_Price_Class.xlsx') X = data[['Date']] y = data[['Ron95_RM']] y = np.ravel(y) logistic.fit(X, y) price = logistic.predict(42491) print 'The price for Ron95 in next month will be RM', np.array_str(price,1) </code></pre> <p>This is the output of the code</p> <pre><code>The price for Ron95 in next month will be RM [ u'B'] </code></pre> <p>There is no error, but my question is the characters after RM in the output should be 'B' or an other characters. I wonder if it's because I do the code wrongly or is just a format problem with the numpy array. </p> <p>Because I basically just started with Python today, sorry if I just made a stupid mistake.</p>",Logistic Regression in python,0,python machine-learning logistic-regression
38621685,,"<p>I have been trying to implement logistic regression for a classification problem, but it is giving me really bizarre results. I have gotten decent results with gradient boosting and random forests so I thought of getting to basic and see what best can I achieve. Can you help me point out what am I doing wrong that is causing this overfitting? You can get the data from <a href='https://www.kaggle.com/c/santander-customer-satisfaction/data' rel='nofollow'>https://www.kaggle.com/c/santander-customer-satisfaction/data</a></p> <p>Here is my code:</p> <pre><code>import pandas as pd import numpy as np train = pd.read_csv('path') test = pd.read_csv('path') test['TARGET'] = 0 fullData = pd.concat([train,test], ignore_index = True) remove1 = [] for col in fullData.columns: if fullData[col].std() == 0: remove1.append(col) fullData.drop(remove1, axis=1, inplace=True) import numpy as np remove = [] cols = fullData.columns for i in range(len(cols)-1): v = fullData[cols[i]].values for j in range(i+1,len(cols)): if np.array_equal(v,fullData[cols[j]].values): remove.append(cols[j]) fullData.drop(remove, axis=1, inplace=True) from sklearn.cross_validation import train_test_split X_train, X_test = train_test_split(fullData, test_size=0.20, random_state=1729) print(X_train.shape, X_test.shape) y_train = X_train['TARGET'].values X = X_train.drop(['TARGET','ID'],axis=1,inplace = False) from sklearn.ensemble import ExtraTreesClassifier clf = ExtraTreesClassifier(random_state=1729) selector = clf.fit(X, y_train) from sklearn.feature_selection import SelectFromModel fs = SelectFromModel(selector, prefit=True) X_t = X_test.drop(['TARGET','ID'],axis=1,inplace = False) X_t = fs.transform(X_t) X_tr = X_train.drop(['TARGET','ID'],axis=1,inplace = False) X_tr = fs.transform(X_tr) from sklearn.linear_model import LogisticRegression log = LogisticRegression(penalty ='l2', C = 1, random_state = 1, ) from sklearn import cross_validation scores = cross_validation.cross_val_score(log,X_tr,y_train,cv = 10) print(scores.mean()) log.fit(X_tr,y_train) predictions = log.predict(X_t) predictions = predictions.astype(int) print(predictions.mean()) </code></pre>",Logistic Regression Python,1,python logistic-regression data-science
40223553,,"<p>I am using the below code for logistic regression with regularization in python. Its giving me 80% accuracy on the training set itself.</p> <p>I am using minimize method 'TNC'. With BFG the results are of 50%. What is the ideal method(equivalent to fminunc in Octave) to use for gradient descent? How can I increase or decrease iteration? What is the default iteration? Any other suggestion/approach to improve performance?</p> <p>The same algo in Octave with fminunc gives 83% accuracy on the training set. </p> <pre><code>import numpy as np import scipy.optimize as op from sklearn import preprocessing import matplotlib.pyplot as plt from matplotlib import style from pylab import scatter, show, legend, xlabel, ylabel from numpy import loadtxt, where from sklearn.preprocessing import PolynomialFeatures def sigmoid(z): return 1/(1 + np.exp(-z)); def Gradient(theta,X,y,l): m,n = X.shape #print('theta shape') #print(theta.shape) theta = theta.reshape((n,1)) thetaR = theta[1:n,:] y = y.reshape((m,1)) h = sigmoid(X.dot(theta)) nonRegGrad = ((np.sum(((h-y)*X),axis=0))/m).reshape(n,1) reg = np.insert((l/m)*thetaR,0,0,axis=0) grad = nonRegGrad + reg return grad.flatten(); def CostFunc(theta,X,y,l): h = sigmoid(X.dot(theta)) m,n=X.shape; #print('theta shape') #print(theta.shape) theta = theta.reshape((n,1)) thetaR = theta[1:n,:] cost=np.sum((np.multiply(-y,np.log(h))-np.multiply((1-y),np.log(1-h))))/m reg=(l/(2*m))* np.sum(np.square(thetaR)) J=cost+reg return J; def predict(theta,X): m,n=X.shape; return np.round(sigmoid(X.dot(theta.reshape(n,1)))); data = np.loadtxt(open('ex2data2.txt','rb'),delimiter=',',skiprows=1) nr,nc = data.shape X=data[:,0:nc - 1] #X=preprocessing.scale(X) #X=np.insert(X,0,1,axis=1) y= data[:,[nc - 1]] pos = where(y == 1) neg = where(y == 0) scatter(X[pos, 0], X[pos, 1], marker='o', c='b') scatter(X[neg, 0], X[neg, 1], marker='x', c='r') xlabel('Microchip Test 1') ylabel('Microchip Test 2') legend(['Passed', 'Failed']) show() storeX=X poly = PolynomialFeatures(6) X=poly.fit_transform(X) #print(X.shape) m , n = X.shape; initial_theta = np.zeros((n,1)); #initial_theta = zeros(shape=(it.shape[1], 1)) l = 1 # Compute and display initial cost and gradient for regularized logistic # regression #cost, grad = cost_function_reg(initial_theta, X, y, l) #def decorated_cost(theta): # return cost_function_reg(theta, X, y, l) #print fmin_bfgs(decorated_cost, initial_theta, maxfun=400) print('Calling optimization') Result = op.minimize(fun = CostFunc, x0 = initial_theta, args = (X, y,l), method = 'TNC', jac = Gradient); optimal_theta = Result.x; print(Result.x.shape) print('optimal theta') print(optimal_theta) p=predict(optimal_theta,X) accuracy = np.mean(np.double(p==y)) print('accuracy') print(accuracy) enter code here </code></pre>",Regularised Logistic regression in Python,1,python machine-learning scipy logistic-regression
40251587,,"<p>I have created repo</p> <p><a href='https://github.com/joyjeni/tensorflowexample.git' rel='nofollow noreferrer'>https://github.com/joyjeni/tensorflowexample.git</a></p> <p>I m trying to do logistic regression on kaggle titanic dataset</p> <p>I get below error</p> <p>OutOfRangeError: RandomShuffleQueue '_398_shuffle_batch_16/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 0)</p> <p>Can someone tell me why current size is 0.</p>",OutOfRangeError while doing logistic regression,0,tensorflow kaggle
40865717,,"<p>I have been at this for a couple of hours and feel really really stuck now.</p> <p>I am trying to use a bunch of columns in a csv 'ScoreBuckets.csv' to predict another column in that csv called 'Score_Bucket'. I would like to use multiple columns in the csv to predict the column Score_Bucket. The problem I am having is that my results don't make any sense at all, and I don't know how to use multiple columns to predict the column Score_Bucket. I am new to data mining, so I am not 100% familiar with the code/syntax. </p> <p>Here is the code I have so far:</p> <pre><code>import pandas as pd import numpy as np from sklearn import metrics from sklearn.linear_model import LogisticRegression from sklearn.cross_validation import KFold, cross_val_score dataset = pd.read_csv('ScoreBuckets.csv') CV = (dataset.Score_Bucket.reshape((len(dataset.Score_Bucket), 1))).ravel() data = (dataset.ix[:,'CourseLoad_RelativeStudy':'Sleep_Sex'].values).reshape( (len(dataset.Score_Bucket), 2)) # Create a KNN object LogReg = LogisticRegression() # Train the model using the training sets LogReg.fit(data, CV) # the model print('Coefficients (m): ', LogReg.coef_) print('Intercept (b): ', LogReg.intercept_) #predict the class for each data point predicted = LogReg.predict(data) print('Predictions: ', np.array([predicted]).T) # predict the probability/likelihood of the prediction print('Probability of prediction: ',LogReg.predict_proba(data)) modelAccuracy = LogReg.score(data,CV) print('Accuracy score for the model: ', LogReg.score(data,CV)) print(metrics.confusion_matrix(CV, predicted, labels=['Yes','No'])) # Calculating 5 fold cross validation results LogReg = LogisticRegression() kf = KFold(len(CV), n_folds=5) scores = cross_val_score(LogReg, data, CV, cv=kf) print('Accuracy of every fold in 5 fold cross validation: ', abs(scores)) print('Mean of the 5 fold cross-validation: %0.2f' % abs(scores.mean())) print('The accuracy difference between model and KFold is: ', abs(abs(scores.mean())-modelAccuracy)) </code></pre> <p>ScoreBuckets.csv:</p> <pre><code>Score_Bucket,Healthy,Course_Load,Miss_Class,Relative_Study,Faculty,Sleep,Relation_Status,Sex,Relative_Stress,Res_Gym?,Tuition_Awareness,Satisfaction,Healthy_TuitionAwareness,Healthy_TuitionAwareness_MissClass,Healthy_MissClass_Sex,Sleep_Faculty_RelativeStress,TuitionAwareness_ResGym,CourseLoad_RelativeStudy,Sleep_Sex 5,0.5,1,0,1,0.4,0.33,1,0,0.5,1,0,0,0.75,0.5,0.17,0.41,0.5,1,0.17 2,1,1,0.33,0.5,0.4,0.33,0,0,1,0,0,0,0.5,0.44,0.44,0.58,0,0.75,0.17 5,0.5,1,0,0.5,0.4,0.33,1,0,0.5,0,1,0,0.75,0.5,0.17,0.41,0.5,0.75,0.17 4,0.5,1,0,0,0.4,0.33,0,0,0.5,0,1,0,0.25,0.17,0.17,0.41,0.5,0.5,0.17 5,0.5,1,0.33,0.5,0.4,0,1,1,1,0,1,0,0.75,0.61,0.61,0.47,0.5,0.75,0.5 5,0.5,1,0,1,0.4,0.33,1,1,1,1,1,1,0.75,0.5,0.5,0.58,1,1,0.67 5,0.5,1,0,0,0.4,0.33,0,0,0.5,0,1,0,0.25,0.17,0.17,0.41,0.5,0.5,0.17 2,0.5,1,0.67,0.5,0.4,0,1,1,0.5,0,0,0,0.75,0.72,0.72,0.3,0,0.75,0.5 5,0.5,1,0,1,0.4,0.33,0,1,1,0,1,1,0.25,0.17,0.5,0.58,0.5,1,0.67 5,1,1,0,0.5,0.4,0.33,0,1,0.5,0,1,1,0.5,0.33,0.67,0.41,0.5,0.75,0.67 0,0.5,1,0,1,0.4,0.33,0,0,0.5,0,0,0,0.25,0.17,0.17,0.41,0,1,0.17 2,0.5,1,0,0.5,0.4,0.33,1,1,1,0,0,0,0.75,0.5,0.5,0.58,0,0.75,0.67 5,0.5,1,0,1,0.4,0.33,0,0,1,1,1,0,0.25,0.17,0.17,0.58,1,1,0.17 0,0.5,1,0.33,0.5,0.4,0.33,1,1,0.5,0,1,0,0.75,0.61,0.61,0.41,0.5,0.75,0.67 5,0.5,1,0,0.5,0.4,0.33,0,0,0.5,0,1,1,0.25,0.17,0.17,0.41,0.5,0.75,0.17 4,0,1,0.67,0.5,0.4,0.67,1,0,0.5,1,0,0,0.5,0.56,0.22,0.52,0.5,0.75,0.34 2,0.5,1,0.33,1,0.4,0.33,0,0,0.5,0,1,0,0.25,0.28,0.28,0.41,0.5,1,0.17 5,0.5,1,0.33,0.5,0.4,0.33,0,1,1,0,1,0,0.25,0.28,0.61,0.58,0.5,0.75,0.67 5,0.5,1,0,1,0.4,0.33,0,0,0.5,1,1,0,0.25,0.17,0.17,0.41,1,1,0.17 5,0.5,1,0.33,0.5,0.4,0.33,1,1,1,0,1,0,0.75,0.61,0.61,0.58,0.5,0.75,0.67 </code></pre> <p>Output:</p> <pre><code>Coefficients (m): [[-0.4012899 -0.51699939] [-0.72785212 -0.55622303] [-0.62116232 0.30564259] [ 0.04222459 -0.01672418]] Intercept (b): [-1.80383738 -1.5156701 -1.29452772 0.67672118] Predictions: [[5] [5] [5] [5] ... [5] [5] [5] [5]] Probability of prediction: [[ 0.09302973 0.08929139 0.13621146 0.68146742] [ 0.09777325 0.10103782 0.14934111 0.65184782] [ 0.09777325 0.10103782 0.14934111 0.65184782] [ 0.10232068 0.11359509 0.16267645 0.62140778] ... [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07346886 0.07417316 0.18264008 0.66971789]] Accuracy score for the model: 0.671171171171 [[0 0] [0 0]] Accuracy of every fold in 5 fold cross validation: [ 0.64444444 0.73333333 0.68181818 0.63636364 0.65909091] Mean of the 5 fold cross-validation: 0.67 The accuracy difference between model and KFold is: 0.00016107016107 </code></pre> <p>The reason I say that the output doesn't make sense is for two reasons: 1. Regardless of what data I feed for the column, the prediction accuracy stays the same and that shouldn't happen because some columns are better predictors of Score_Buckets column. 2. It won't let me use multiple columns to predict the column Score_Buckets because it says they have to be the same size, but how can that be when multiple columns would obviously have a larger array size than only the column Score_Buckets.</p> <p>What am I doing wrong with the prediction?</p>",Python Logistic Regression,0,python data-mining logistic-regression
46349963,,"<p>I'm trying to build a Logistic regression model using numPy and training it on TensorFlow 'Getting Started' example: <code>{x: [1, 2, 3, 4], y: [0, -1, -2, -3]}</code> using the same learning rate and epochs as the one on tensorFlow example but for some reason it cant learn the correct weight and bias. Any help? I'm new to AI.</p> <p>Code:</p> <pre><code># Compute cost and gradient def propagate(w, b, X, Y): m = X.shape[0] A = sigmoid(np.multiply(w,X) + b) arr = (np.multiply(w,X) + b) - Y cost = np.dot(arr, arr) cost = np.squeeze(cost) dw = 1/m * X.dot((A-Y).T) db = 1/m * np.sum(A-Y) return {'db': db, 'dw': dw}, cost # Gradient Descnet def optimize(w, b, X, Y, epochs, learning_rate): costs = [] for i in range(epochs): grads, cost = propagate(w, b, X, Y) dw = grads['dw'] db = grads['db'] w = w - learning_rate * dw b = b - learning_rate * db if i % 100 == 0: costs.append(cost) return {'w':w, 'b':b}, {'db': db, 'dw': dw}, costs </code></pre> <p>Output:</p> <pre><code>w, b, X, Y = np.array([0.3]), -0.3, np.array([1, 2, 3, 4]), np.array([0, -1, -2, -3]) grads, cost = propagate(w, b, X, Y) print ('dw = ' + str(grads['dw'])) # dw = 6.6074129907 print ('db = ' + str(grads['db'])) # db = 2.10776208142 print ('cost = ' + str(cost)) # cost = 23.66 params, grads, costs = optimize(w, b, X, Y, epochs= 100, learning_rate = 0.01) print ('w = ' + str(params['w'])) # w = [-4.85038348] (supposed to be about -0.9999969) print ('b = ' + str(params['b'])) # b = -1.86763966366 (supposed to be about 0.99999082) </code></pre>",Logistic Regression in Python,0,python numpy tensorflow artificial-intelligence logistic-regression
48962697,,"<p>I created a simple Logistic Regression model using Python and Chainer but I am not fully satisfied with the end result. Therefore, I would like to get some help. One restriction: I would not like to interchange the implemented functionalities by already existing functionalities. I know that there are loss functions in Chainer which archieve almost the same, but a more complex model I am creating is using a custom loss function. The code is found here:</p> <p><a href='https://gist.github.com/kmjjacobs/62fc96ece695b47af8d667b060a64559' rel='nofollow noreferrer'>https://gist.github.com/kmjjacobs/62fc96ece695b47af8d667b060a64559</a></p> <p>I would like to keep the model code as clean as possible, but as you can see, the <strong>call</strong> method is a forward to the loss method and I suspect there is a cleaner way to invoke the loss method in the training loop. I thought that it would be cleaner if the <strong>call</strong> method outputs the prediction and there is a seperate loss method for computing the loss. What are your thoughts on this?</p> <p>I am also not sure on the converter function. Is there a better way to achieve the same result?</p> <p>Do you have any remarks or best practices for writing Chainer code?</p> <p>Thanks in advance!</p>",Chainer - Python - Logistic Regression,0,python machine-learning artificial-intelligence logistic-regression chainer
49135107,,"<p>I have a dataset consisting of student marks in 2 subjects and the result if the student is admitted in college or not. I need to perform a logistic regression on the data and find the optimum parameter θ to minimize the loss and predict the results for the test data. I am not trying to build any complex non linear network here.</p> <p>The data looks like this <a href='https://i.stack.imgur.com/eTN2D.png' rel='nofollow noreferrer'><img src='https://i.stack.imgur.com/eTN2D.png' alt='enter image description here'></a></p> <p>I have the loss function defined for logistic regression like this which works fine</p> <pre><code>predict(X) = sigmoid(X*θ) loss(X,y) = (1 / length(y)) * sum(-y .* log.(predict(X)) .- (1 - y) .* log.(1 - predict(X))) </code></pre> <p>I need to minimize this loss function and find the optimum θ. I want to do it with Flux.jl or any other library which makes it even easier. I tried using Flux.jl after reading the examples but not able to minimize the cost.</p> <p>My code snippet:</p> <pre><code>function update!(ps, η = .1) for w in ps w.data .-= w.grad .* η print(w.data) w.grad .= 0 end end for i = 1:400 back!(L) update!((θ, b)) @show L end </code></pre>",Logistic regression using Flux.jl,1,machine-learning julia-lang logistic-regression
