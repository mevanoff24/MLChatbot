comment_id,comment_parent,title,score_parent,distances,parent_id,comment_comment,score_comment
40251587,"<p>I have created repo</p> <p><a href='https://github.com/joyjeni/tensorflowexample.git' rel='nofollow noreferrer'>https://github.com/joyjeni/tensorflowexample.git</a></p> <p>I m trying to do logistic regression on kaggle titanic dataset</p> <p>I get below error</p> <p>OutOfRangeError: RandomShuffleQueue '_398_shuffle_batch_16/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 0)</p> <p>Can someone tell me why current size is 0.</p>",OutOfRangeError while doing logistic regression,0,0.0,40251587.0,"<p>I assume you are trying to execute the logistic regression example found in the <em>Tensorflow for Machine Intelligence</em> book.</p> <p>When the csv file is loaded, the path is incorrect:</p> <p>Instead of using </p> <p><code>filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + '/' + file_name])</code></p> <p>use </p> <pre><code>filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)]) </code></pre> <p>The first line will load the file '/train.csv' while the second one will load 'yourPath/train.csv'. Due to the fact that the first file doesn't exists, no data to train can be loaded.</p> <p>The source code of all of the book's examples can be found in the following repo:</p> <p><a href='https://github.com/backstopmedia/tensorflowbook' rel='nofollow noreferrer'>https://github.com/backstopmedia/tensorflowbook</a></p>",0.0
49135107,"<p>I have a dataset consisting of student marks in 2 subjects and the result if the student is admitted in college or not. I need to perform a logistic regression on the data and find the optimum parameter θ to minimize the loss and predict the results for the test data. I am not trying to build any complex non linear network here.</p> <p>The data looks like this <a href='https://i.stack.imgur.com/eTN2D.png' rel='nofollow noreferrer'><img src='https://i.stack.imgur.com/eTN2D.png' alt='enter image description here'></a></p> <p>I have the loss function defined for logistic regression like this which works fine</p> <pre><code>predict(X) = sigmoid(X*θ) loss(X,y) = (1 / length(y)) * sum(-y .* log.(predict(X)) .- (1 - y) .* log.(1 - predict(X))) </code></pre> <p>I need to minimize this loss function and find the optimum θ. I want to do it with Flux.jl or any other library which makes it even easier. I tried using Flux.jl after reading the examples but not able to minimize the cost.</p> <p>My code snippet:</p> <pre><code>function update!(ps, η = .1) for w in ps w.data .-= w.grad .* η print(w.data) w.grad .= 0 end end for i = 1:400 back!(L) update!((θ, b)) @show L end </code></pre>",Logistic regression using Flux.jl,1,0.06684142351150513,49135107.0,"<p>You can use either GLM.jl (simpler) or Flux.jl (more involved but more powerful in general). In the code I generate the data so that you can check if the result is correct. Additionally I have a binary response variable - if you have other encoding of target variable you might need to change the code a bit.</p> <p>Here is the code to run (you can tweak the parameters to increase the convergence speed - I chose ones that are safe):</p> <pre><code>using GLM, DataFrames, Flux.Tracker srand(1) n = 10000 df = DataFrame(s1=rand(n), s2=rand(n)) df[:y] = rand(n) .< 1 ./ (1 .+ exp.(-(1 .+ 2 .* df[1] .+ 0.5 .* df[2]))) model = glm(@formula(y~s1+s2), df, Binomial(), LogitLink()) x = Matrix(df[1:2]) y = df[3] W = param(rand(2,1)) b = param(rand(1)) predict(x) = 1.0 ./ (1.0+exp.(-x*W .- b)) loss(x,y) = -sum(log.(predict(x[y,:]))) - sum(log.(1 - predict(x[.!y,:]))) function update!(ps, η = .0001) for w in ps w.data .-= w.grad .* η w.grad .= 0 end end i = 1 while true back!(loss(x,y)) max(maximum(abs.(W.grad)), abs(b.grad[1])) > 0.001 || break update!((W, b)) i += 1 end </code></pre> <p>And here are the results:</p> <pre><code>julia> model # GLM result StatsModels.DataFrameRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} Formula: y ~ 1 + s1 + s2 Coefficients: Estimate Std.Error z value Pr(>|z|) (Intercept) 0.910347 0.0789283 11.5338 <1e-30 s1 2.18707 0.123487 17.7109 <1e-69 s2 0.556293 0.115052 4.83513 <1e-5 julia> (b, W, i) # Flux result with number of iterations needed to converge (param([0.910362]), param([2.18705; 0.556278]), 1946) </code></pre>",3.0
37391923,"<p>I am currently doing the Logistic Regression in machine learning for python. This is the code i write.</p> <pre><code>import pandas as pd from sklearn import linear_model import numpy as np from sklearn.utils import column_or_1d logistic = linear_model.LogisticRegression() data = pd.read_excel('/home/mick/PycharmProjects/project1/excel/Ron95_Price_Class.xlsx') X = data[['Date']] y = data[['Ron95_RM']] y = np.ravel(y) logistic.fit(X, y) price = logistic.predict(42491) print 'The price for Ron95 in next month will be RM', np.array_str(price,1) </code></pre> <p>This is the output of the code</p> <pre><code>The price for Ron95 in next month will be RM [ u'B'] </code></pre> <p>There is no error, but my question is the characters after RM in the output should be 'B' or an other characters. I wonder if it's because I do the code wrongly or is just a format problem with the numpy array. </p> <p>Because I basically just started with Python today, sorry if I just made a stupid mistake.</p>",Logistic Regression in python,0,0.166580468416214,37391923.0,"<p>I think it will be more easily, when you post some data from Ron95_Price_Class.xlsx<br> Right now I see, that you are not delete target variable (y), from train data. You can do it by </p> <pre><code>X = data['Date'] #you can use only one bracket if choose only y = data['Ron95_RM'] #column X = data.drop('Ron95_RM') </code></pre>",0.0
37391923,"<p>I am currently doing the Logistic Regression in machine learning for python. This is the code i write.</p> <pre><code>import pandas as pd from sklearn import linear_model import numpy as np from sklearn.utils import column_or_1d logistic = linear_model.LogisticRegression() data = pd.read_excel('/home/mick/PycharmProjects/project1/excel/Ron95_Price_Class.xlsx') X = data[['Date']] y = data[['Ron95_RM']] y = np.ravel(y) logistic.fit(X, y) price = logistic.predict(42491) print 'The price for Ron95 in next month will be RM', np.array_str(price,1) </code></pre> <p>This is the output of the code</p> <pre><code>The price for Ron95 in next month will be RM [ u'B'] </code></pre> <p>There is no error, but my question is the characters after RM in the output should be 'B' or an other characters. I wonder if it's because I do the code wrongly or is just a format problem with the numpy array. </p> <p>Because I basically just started with Python today, sorry if I just made a stupid mistake.</p>",Logistic Regression in python,0,0.166580468416214,37391923.0,"<p>If I am not mistaken the 'u' is just notation that the string is a unicode string. I am not sure how you are running your code, but when i test in an ipython notebook or in a windows command prompt I get the following output:</p> <pre><code>The price for Ron95 in next month will be RM [ 'B'] </code></pre> <p>This is perhaps because I ran this in python 3.5 whereas it appears you are still using python < 3.0. </p> <p>It's not that your answer is wrong, you are just getting info about the format of the data. For other questions on this subject see <a href='https://stackoverflow.com/questions/2464959/whats-the-u-prefix-in-a-python-string'>here</a> and <a href='https://stackoverflow.com/questions/11279331/what-does-the-u-symbol-mean-in-front-of-string-values'>here</a>. The python <a href='https://docs.python.org/3/howto/unicode.html' rel='nofollow noreferrer'>how-to on unicode</a> may also be helpful. </p>",0.0
37391923,"<p>I am currently doing the Logistic Regression in machine learning for python. This is the code i write.</p> <pre><code>import pandas as pd from sklearn import linear_model import numpy as np from sklearn.utils import column_or_1d logistic = linear_model.LogisticRegression() data = pd.read_excel('/home/mick/PycharmProjects/project1/excel/Ron95_Price_Class.xlsx') X = data[['Date']] y = data[['Ron95_RM']] y = np.ravel(y) logistic.fit(X, y) price = logistic.predict(42491) print 'The price for Ron95 in next month will be RM', np.array_str(price,1) </code></pre> <p>This is the output of the code</p> <pre><code>The price for Ron95 in next month will be RM [ u'B'] </code></pre> <p>There is no error, but my question is the characters after RM in the output should be 'B' or an other characters. I wonder if it's because I do the code wrongly or is just a format problem with the numpy array. </p> <p>Because I basically just started with Python today, sorry if I just made a stupid mistake.</p>",Logistic Regression in python,0,0.166580468416214,37391923.0,"<p>The Predict method as mentioned in the scikit-learn documentation, <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict' rel='nofollow'>http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict</a>, mentions the return of the predict method is array, shape = [n_samples]. So for you the shape is 1x1 array. To get the desired output you ca try 'price[0]'.</p>",0.0
38621685,"<p>I have been trying to implement logistic regression for a classification problem, but it is giving me really bizarre results. I have gotten decent results with gradient boosting and random forests so I thought of getting to basic and see what best can I achieve. Can you help me point out what am I doing wrong that is causing this overfitting? You can get the data from <a href='https://www.kaggle.com/c/santander-customer-satisfaction/data' rel='nofollow'>https://www.kaggle.com/c/santander-customer-satisfaction/data</a></p> <p>Here is my code:</p> <pre><code>import pandas as pd import numpy as np train = pd.read_csv('path') test = pd.read_csv('path') test['TARGET'] = 0 fullData = pd.concat([train,test], ignore_index = True) remove1 = [] for col in fullData.columns: if fullData[col].std() == 0: remove1.append(col) fullData.drop(remove1, axis=1, inplace=True) import numpy as np remove = [] cols = fullData.columns for i in range(len(cols)-1): v = fullData[cols[i]].values for j in range(i+1,len(cols)): if np.array_equal(v,fullData[cols[j]].values): remove.append(cols[j]) fullData.drop(remove, axis=1, inplace=True) from sklearn.cross_validation import train_test_split X_train, X_test = train_test_split(fullData, test_size=0.20, random_state=1729) print(X_train.shape, X_test.shape) y_train = X_train['TARGET'].values X = X_train.drop(['TARGET','ID'],axis=1,inplace = False) from sklearn.ensemble import ExtraTreesClassifier clf = ExtraTreesClassifier(random_state=1729) selector = clf.fit(X, y_train) from sklearn.feature_selection import SelectFromModel fs = SelectFromModel(selector, prefit=True) X_t = X_test.drop(['TARGET','ID'],axis=1,inplace = False) X_t = fs.transform(X_t) X_tr = X_train.drop(['TARGET','ID'],axis=1,inplace = False) X_tr = fs.transform(X_tr) from sklearn.linear_model import LogisticRegression log = LogisticRegression(penalty ='l2', C = 1, random_state = 1, ) from sklearn import cross_validation scores = cross_validation.cross_val_score(log,X_tr,y_train,cv = 10) print(scores.mean()) log.fit(X_tr,y_train) predictions = log.predict(X_t) predictions = predictions.astype(int) print(predictions.mean()) </code></pre>",Logistic Regression Python,1,0.166580468416214,38621685.0,"<p>You are not configuring the C parameter - well, technically you are, but only to the default value - which is one of the usual suspects for overfitting. You can have a look at <a href='http://scikit-learn.org/stable/modules/grid_search.html#grid-search' rel='nofollow'>GridSearchCV</a> and play around a bit with several values for the C parameter (e.g. from 10^-5 to 10^5) to see if it eases your problem. Changing the penalty rule to 'l1' might help as well.</p> <p>Besides, there were several challenges with that competition: It is an imbalanced data set, and the distributions between the training set and the private LB were a bit different. All of this if going to play against you, specially when using simple algorithms like LR.</p>",0.0
40865717,"<p>I have been at this for a couple of hours and feel really really stuck now.</p> <p>I am trying to use a bunch of columns in a csv 'ScoreBuckets.csv' to predict another column in that csv called 'Score_Bucket'. I would like to use multiple columns in the csv to predict the column Score_Bucket. The problem I am having is that my results don't make any sense at all, and I don't know how to use multiple columns to predict the column Score_Bucket. I am new to data mining, so I am not 100% familiar with the code/syntax. </p> <p>Here is the code I have so far:</p> <pre><code>import pandas as pd import numpy as np from sklearn import metrics from sklearn.linear_model import LogisticRegression from sklearn.cross_validation import KFold, cross_val_score dataset = pd.read_csv('ScoreBuckets.csv') CV = (dataset.Score_Bucket.reshape((len(dataset.Score_Bucket), 1))).ravel() data = (dataset.ix[:,'CourseLoad_RelativeStudy':'Sleep_Sex'].values).reshape( (len(dataset.Score_Bucket), 2)) # Create a KNN object LogReg = LogisticRegression() # Train the model using the training sets LogReg.fit(data, CV) # the model print('Coefficients (m): ', LogReg.coef_) print('Intercept (b): ', LogReg.intercept_) #predict the class for each data point predicted = LogReg.predict(data) print('Predictions: ', np.array([predicted]).T) # predict the probability/likelihood of the prediction print('Probability of prediction: ',LogReg.predict_proba(data)) modelAccuracy = LogReg.score(data,CV) print('Accuracy score for the model: ', LogReg.score(data,CV)) print(metrics.confusion_matrix(CV, predicted, labels=['Yes','No'])) # Calculating 5 fold cross validation results LogReg = LogisticRegression() kf = KFold(len(CV), n_folds=5) scores = cross_val_score(LogReg, data, CV, cv=kf) print('Accuracy of every fold in 5 fold cross validation: ', abs(scores)) print('Mean of the 5 fold cross-validation: %0.2f' % abs(scores.mean())) print('The accuracy difference between model and KFold is: ', abs(abs(scores.mean())-modelAccuracy)) </code></pre> <p>ScoreBuckets.csv:</p> <pre><code>Score_Bucket,Healthy,Course_Load,Miss_Class,Relative_Study,Faculty,Sleep,Relation_Status,Sex,Relative_Stress,Res_Gym?,Tuition_Awareness,Satisfaction,Healthy_TuitionAwareness,Healthy_TuitionAwareness_MissClass,Healthy_MissClass_Sex,Sleep_Faculty_RelativeStress,TuitionAwareness_ResGym,CourseLoad_RelativeStudy,Sleep_Sex 5,0.5,1,0,1,0.4,0.33,1,0,0.5,1,0,0,0.75,0.5,0.17,0.41,0.5,1,0.17 2,1,1,0.33,0.5,0.4,0.33,0,0,1,0,0,0,0.5,0.44,0.44,0.58,0,0.75,0.17 5,0.5,1,0,0.5,0.4,0.33,1,0,0.5,0,1,0,0.75,0.5,0.17,0.41,0.5,0.75,0.17 4,0.5,1,0,0,0.4,0.33,0,0,0.5,0,1,0,0.25,0.17,0.17,0.41,0.5,0.5,0.17 5,0.5,1,0.33,0.5,0.4,0,1,1,1,0,1,0,0.75,0.61,0.61,0.47,0.5,0.75,0.5 5,0.5,1,0,1,0.4,0.33,1,1,1,1,1,1,0.75,0.5,0.5,0.58,1,1,0.67 5,0.5,1,0,0,0.4,0.33,0,0,0.5,0,1,0,0.25,0.17,0.17,0.41,0.5,0.5,0.17 2,0.5,1,0.67,0.5,0.4,0,1,1,0.5,0,0,0,0.75,0.72,0.72,0.3,0,0.75,0.5 5,0.5,1,0,1,0.4,0.33,0,1,1,0,1,1,0.25,0.17,0.5,0.58,0.5,1,0.67 5,1,1,0,0.5,0.4,0.33,0,1,0.5,0,1,1,0.5,0.33,0.67,0.41,0.5,0.75,0.67 0,0.5,1,0,1,0.4,0.33,0,0,0.5,0,0,0,0.25,0.17,0.17,0.41,0,1,0.17 2,0.5,1,0,0.5,0.4,0.33,1,1,1,0,0,0,0.75,0.5,0.5,0.58,0,0.75,0.67 5,0.5,1,0,1,0.4,0.33,0,0,1,1,1,0,0.25,0.17,0.17,0.58,1,1,0.17 0,0.5,1,0.33,0.5,0.4,0.33,1,1,0.5,0,1,0,0.75,0.61,0.61,0.41,0.5,0.75,0.67 5,0.5,1,0,0.5,0.4,0.33,0,0,0.5,0,1,1,0.25,0.17,0.17,0.41,0.5,0.75,0.17 4,0,1,0.67,0.5,0.4,0.67,1,0,0.5,1,0,0,0.5,0.56,0.22,0.52,0.5,0.75,0.34 2,0.5,1,0.33,1,0.4,0.33,0,0,0.5,0,1,0,0.25,0.28,0.28,0.41,0.5,1,0.17 5,0.5,1,0.33,0.5,0.4,0.33,0,1,1,0,1,0,0.25,0.28,0.61,0.58,0.5,0.75,0.67 5,0.5,1,0,1,0.4,0.33,0,0,0.5,1,1,0,0.25,0.17,0.17,0.41,1,1,0.17 5,0.5,1,0.33,0.5,0.4,0.33,1,1,1,0,1,0,0.75,0.61,0.61,0.58,0.5,0.75,0.67 </code></pre> <p>Output:</p> <pre><code>Coefficients (m): [[-0.4012899 -0.51699939] [-0.72785212 -0.55622303] [-0.62116232 0.30564259] [ 0.04222459 -0.01672418]] Intercept (b): [-1.80383738 -1.5156701 -1.29452772 0.67672118] Predictions: [[5] [5] [5] [5] ... [5] [5] [5] [5]] Probability of prediction: [[ 0.09302973 0.08929139 0.13621146 0.68146742] [ 0.09777325 0.10103782 0.14934111 0.65184782] [ 0.09777325 0.10103782 0.14934111 0.65184782] [ 0.10232068 0.11359509 0.16267645 0.62140778] ... [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07920945 0.08045552 0.17396476 0.66637027] [ 0.07346886 0.07417316 0.18264008 0.66971789]] Accuracy score for the model: 0.671171171171 [[0 0] [0 0]] Accuracy of every fold in 5 fold cross validation: [ 0.64444444 0.73333333 0.68181818 0.63636364 0.65909091] Mean of the 5 fold cross-validation: 0.67 The accuracy difference between model and KFold is: 0.00016107016107 </code></pre> <p>The reason I say that the output doesn't make sense is for two reasons: 1. Regardless of what data I feed for the column, the prediction accuracy stays the same and that shouldn't happen because some columns are better predictors of Score_Buckets column. 2. It won't let me use multiple columns to predict the column Score_Buckets because it says they have to be the same size, but how can that be when multiple columns would obviously have a larger array size than only the column Score_Buckets.</p> <p>What am I doing wrong with the prediction?</p>",Python Logistic Regression,0,0.166580468416214,40865717.0,"<p>First of all, double-check if your problem can really be framed as a classification problem or if it should rather be formulated as a regression problem.</p> <p>Assuming you really want to classify your data into the four unique classes present in the <code>Score_Bucket</code> column, why do you think you cannot use multiple columns as predictors? In fact, you are using the last two columns in your example. You can make your code a bit more readable if you consider that <code>sklearn</code> methods directly work with Pandas DataFrames (no need for converting to NumPy arrays):</p> <pre><code>X = dataset[['CourseLoad_RelativeStudy', 'Sleep_Sex']] y = dataset[['Score_Bucket']] logreg = LogisticRegression() logreg.fit(X, y) </code></pre> <p>If you want to select more columns, you can use the <code>loc</code> method:</p> <pre><code>X = dataset.loc[:, 'Healthy':'Sleep_Sex'] </code></pre> <p>You could also select columns by index:</p> <pre><code>X = dataset.iloc[:, 1:] </code></pre> <p>Regarding your second question, I do get different results from the cross-validation procedure depending on which columns I use as features. Just note that you have a very low number of samples (20), which makes your estimated predictions rather variable.</p>",1.0
46349963,"<p>I'm trying to build a Logistic regression model using numPy and training it on TensorFlow 'Getting Started' example: <code>{x: [1, 2, 3, 4], y: [0, -1, -2, -3]}</code> using the same learning rate and epochs as the one on tensorFlow example but for some reason it cant learn the correct weight and bias. Any help? I'm new to AI.</p> <p>Code:</p> <pre><code># Compute cost and gradient def propagate(w, b, X, Y): m = X.shape[0] A = sigmoid(np.multiply(w,X) + b) arr = (np.multiply(w,X) + b) - Y cost = np.dot(arr, arr) cost = np.squeeze(cost) dw = 1/m * X.dot((A-Y).T) db = 1/m * np.sum(A-Y) return {'db': db, 'dw': dw}, cost # Gradient Descnet def optimize(w, b, X, Y, epochs, learning_rate): costs = [] for i in range(epochs): grads, cost = propagate(w, b, X, Y) dw = grads['dw'] db = grads['db'] w = w - learning_rate * dw b = b - learning_rate * db if i % 100 == 0: costs.append(cost) return {'w':w, 'b':b}, {'db': db, 'dw': dw}, costs </code></pre> <p>Output:</p> <pre><code>w, b, X, Y = np.array([0.3]), -0.3, np.array([1, 2, 3, 4]), np.array([0, -1, -2, -3]) grads, cost = propagate(w, b, X, Y) print ('dw = ' + str(grads['dw'])) # dw = 6.6074129907 print ('db = ' + str(grads['db'])) # db = 2.10776208142 print ('cost = ' + str(cost)) # cost = 23.66 params, grads, costs = optimize(w, b, X, Y, epochs= 100, learning_rate = 0.01) print ('w = ' + str(params['w'])) # w = [-4.85038348] (supposed to be about -0.9999969) print ('b = ' + str(params['b'])) # b = -1.86763966366 (supposed to be about 0.99999082) </code></pre>",Logistic Regression in Python,0,0.166580468416214,46349963.0,<p>You can just use a linear function as activation for your last layer. And it is better to normalize your data to have better results.</p>,0.0
48962697,"<p>I created a simple Logistic Regression model using Python and Chainer but I am not fully satisfied with the end result. Therefore, I would like to get some help. One restriction: I would not like to interchange the implemented functionalities by already existing functionalities. I know that there are loss functions in Chainer which archieve almost the same, but a more complex model I am creating is using a custom loss function. The code is found here:</p> <p><a href='https://gist.github.com/kmjjacobs/62fc96ece695b47af8d667b060a64559' rel='nofollow noreferrer'>https://gist.github.com/kmjjacobs/62fc96ece695b47af8d667b060a64559</a></p> <p>I would like to keep the model code as clean as possible, but as you can see, the <strong>call</strong> method is a forward to the loss method and I suspect there is a cleaner way to invoke the loss method in the training loop. I thought that it would be cleaner if the <strong>call</strong> method outputs the prediction and there is a seperate loss method for computing the loss. What are your thoughts on this?</p> <p>I am also not sure on the converter function. Is there a better way to achieve the same result?</p> <p>Do you have any remarks or best practices for writing Chainer code?</p> <p>Thanks in advance!</p>",Chainer - Python - Logistic Regression,0,0.166580468416214,48962697.0,"<p>At first what is your main question? The best way to define loss function and predict function separately?</p> <p>I looked your code, I think functionality of <code>init_scope</code> is different between <code>Link</code> and <code>Chain</code>. You cannot use it to register learnable parameter in Chain for this purpose. (Your current usage is for the <code>Link</code> and not for the <code>Chain</code>.)</p> <ul> <li><p><code>init_scope</code> in <code>Link</code> is used to register parameter, <a href='https://docs.chainer.org/en/stable/tutorial/function.html#links-that-wrap-functions' rel='nofollow noreferrer'>https://docs.chainer.org/en/stable/tutorial/function.html#links-that-wrap-functions</a> <a href='https://github.com/chainer/chainer/blob/master/chainer/link.py#L197' rel='nofollow noreferrer'>https://github.com/chainer/chainer/blob/master/chainer/link.py#L197</a></p></li> <li><p><code>init_scope</code> in <code>Chain</code> is used to register other <code>links</code>, <a href='https://docs.chainer.org/en/stable/tutorial/basic.html#write-a-model-as-a-chain' rel='nofollow noreferrer'>https://docs.chainer.org/en/stable/tutorial/basic.html#write-a-model-as-a-chain</a> <a href='https://github.com/chainer/chainer/blob/master/chainer/link.py#L675' rel='nofollow noreferrer'>https://github.com/chainer/chainer/blob/master/chainer/link.py#L675</a></p></li> </ul> <p>In your case, I think you can just use <code>chainer.links.Linear</code> to your <code>LogisticRegressionModel</code>, or you can define your own <code>Link</code> class which has the learnable parameter <code>W</code> and use this own <code>link</code> class in your <code>LogisticRegressionModel</code>.</p>",1.0
20328835,"<p>I want to implement Logisitic regression from scratch in python. Following are the functions in it: </p> <ol> <li>sigmoid</li> <li>cost </li> <li>fminunc</li> <li>Evaluating Logistic regression</li> </ol> <p>I would like to know, what would be a great start to this to start from scratch in python. Any guidance on how and what would be good. I know the theory of those functions but looking for a better pythonic answer.</p> <p>I used octave and I got it all right but dont know how to start in python as OCtave already has those packages setup to do the work.</p>",Logistic regression using python,1,0.17440785467624664,20328835.0,<p>You may want to try to translate your octave code to python and see what's going on. You can also use the python package to do this for you. Check out <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html' rel='nofollow'>scikit-learn</a> on logistic regression. There is also an easy example in this <a href='http://blog.yhathq.com/posts/logistic-regression-and-python.html' rel='nofollow'>blog</a>.</p>,2.0
20328835,"<p>I want to implement Logisitic regression from scratch in python. Following are the functions in it: </p> <ol> <li>sigmoid</li> <li>cost </li> <li>fminunc</li> <li>Evaluating Logistic regression</li> </ol> <p>I would like to know, what would be a great start to this to start from scratch in python. Any guidance on how and what would be good. I know the theory of those functions but looking for a better pythonic answer.</p> <p>I used octave and I got it all right but dont know how to start in python as OCtave already has those packages setup to do the work.</p>",Logistic regression using python,1,0.17440785467624664,20328835.0,"<p>In order to implement Logistic Regression, You may consider the following 2 approaches:</p> <ol> <li><p>Consider How Linear Regression Works. Apply Sigmoid Function to the Hypothesis of Linear Regression and run gradient Descent until convergence. OR Apply the Exponential based Softmax function to rule out lower possibility of occurrence. </p> <pre><code>def logistic_regression(x, y,alpha=0.05,lamda=0): ''' Logistic regression for datasets ''' m,n=np.shape(x) theta=np.ones(n) xTrans = x.transpose() oldcost=0.0 value=True while(value): hypothesis = np.dot(x, theta) logistic=hypothesis/(np.exp(-hypothesis)+1) reg = (lamda/2*m)*np.sum(np.power(theta,2)) loss = logistic - y cost = np.sum(loss ** 2) #print(cost) # avg cost per example (the 2 in 2*m doesn't really matter here. # But to be consistent with the gradient, I include it) # avg gradient per example gradient = np.dot(xTrans, loss)/m # update if(reg): cost=cost+reg theta = (theta - (alpha) * (gradient+reg)) else: theta=theta -(alpha/m) * gradient if(oldcost==cost): value=False else: oldcost=cost print(accuracy(theta,m,y,x)) return theta,accuracy(theta,m,y,x) </code></pre></li> </ol>",1.0
33616265,"<p>I am learning Machine Learning, and so far I have used logistic regression with problems with balanced data like sentiment analysis where I had equal number of training data for both classes (+ve,-ve).</p> <p>Now, I am working on Named Entity Recognition problem where I have to identify names of people in the text. For this my data is sparse, less than 10% of my training data is +ve case(actually a person tag), reset is negative case(not a person tag). So there is massive imbalance in my training data.</p> <p>Will a supervised learning algorithm work in this scenario?</p>",Will logistic regression work with imblanced data?,2,0.2624742388725281,33616265.0,"<p>Yes; it will work fine, so long as you have enough data on each side to properly define the class. The amount you need depends on the classification method you use. In fact, I have a couple of SVM models that work very nicely, trained with nothing but +ve data -- no -ve data at all!</p> <p>For most methods, the lopsided input suggests that you <em>could</em> toss out the 80% of your +ve cases that aren't doing as much to define the boundary. Which 80% will vary with the method. For instance, spectral clustering and k-means will work well enough if you remove 80% evenly spaced (at random is likely to work). Linear SVM works if you keep only the 10% nearest the boundary. Naive Bayes and random forest can also work nicely with a random 80% removal, although any of these that work by successive refinement may converge a little more slowly.</p>",1.0
33616265,"<p>I am learning Machine Learning, and so far I have used logistic regression with problems with balanced data like sentiment analysis where I had equal number of training data for both classes (+ve,-ve).</p> <p>Now, I am working on Named Entity Recognition problem where I have to identify names of people in the text. For this my data is sparse, less than 10% of my training data is +ve case(actually a person tag), reset is negative case(not a person tag). So there is massive imbalance in my training data.</p> <p>Will a supervised learning algorithm work in this scenario?</p>",Will logistic regression work with imblanced data?,2,0.2624742388725281,33616265.0,"<p>It all depends on your results. </p> <hr> <p>Extreme scenario aside: Consider the following scenario: You run your model, and observe your error is somewhat low like 5%. However, in reality, this 5% was because you wrongly classified HALF of your negative data (failed to recognize person's name), then obviously 5% error seems much worse now.</p> <hr> <p>One thing you should do is calculate your precision and recall. </p> <p><strong>Precision</strong>: </p> <p>Out of all the words that we predicted are peoples' names, what fraction of them <em>actually</em> are?</p> <p>Precision = # true positives / (# of true positives + # of false positives)</p> <p>High precision (close to 1) means you have few # of false positives. In other words, your model is pulling a Dos Equis: 'I don't always predict positive, but when I do, I get it right'</p> <p>However, high precision does not tell you whether you classified ALL actual positive examples correctly.</p> <hr> <p><strong>Recall</strong>:</p> <p>Of all the examples that are <em>actually</em> peoples' names, what fraction of those did we predict <em>correctly</em>?</p> <p>Recall = # true positives / (# true positives + # false negatives)</p> <p>High recall (close to 1) means we are correctly classifying the positive examples, and moreover, not misclassifying negative examples</p> <p>However, high recall doesn't tell you whether your model misclassified some negative examples as positives (false positives)</p> <hr> <p>Precision and recall can be tradeoffs (could have lots of one at the expense of the other). If so, look into how to calculate F1 score (simple), which determines whether your model has a sufficient amount of precision and recall.</p> <p>A good F1 score is close to 1. This should tell you more about how well your model is classifying text as people names.</p>",1.0
